{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most tokenization schemes have two parts: a token learner, and a token seg-menter. The token learner takes a raw training corpus (sometimes roughly pre\n",
    "separated into words, for example by whitespace) and induces a vocabulary, a set\n",
    " of tokens. The token segmenter takes a raw test sentence and segments it into the\n",
    " tokens in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Byte-Pair Encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BPE token learner begins\n",
    " with a vocabulary that is just the set of all individual characters. It then examines the\n",
    " training corpus, chooses the two symbols that are most frequently adjacent (say ‘A’,\n",
    " ‘B’), adds a new merged symbol ‘AB’ to the vocabulary, and replaces every adjacent\n",
    " ’A’ ’B’ in the corpus with the new ‘AB’. It continues to count and merge, creating\n",
    " new longer and longer character strings, until k merges have been done creating\n",
    " k novel tokens; k is thus a parameter of the algorithm. The resulting vocabulary\n",
    " consists of the original set of characters plus k new symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the example that walks through creating a vocabulary, counting pair frequencies, and iteratively merging the most frequent pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus: a list of words.\n",
    "\n",
    "corpus = [\"low_\", \"lower_\", \"lowest_\", \"new_\", \"newer_\", \"newest_\", \"wide_\", \"wider_\", \"widest_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to retrieve unique chars\n",
    "def get_unique_chars(corpus):\n",
    "    unique_chars = set()\n",
    "\n",
    "    # iterate over each word in the corpus\n",
    "    for word in corpus:\n",
    "        unique_chars.update(list(word))\n",
    "    return sorted(unique_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Characters: ['_', 'd', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# sort the retrieved characters\n",
    "sorted_unique_chars = get_unique_chars(corpus)\n",
    "\n",
    "print(\"Unique Characters:\", sorted_unique_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Vocabulary:\n",
      "l o w _ : 1\n",
      "l o w e r _ : 1\n",
      "l o w e s t _ : 1\n",
      "n e w _ : 1\n",
      "n e w e r _ : 1\n",
      "n e w e s t _ : 1\n",
      "w i d e _ : 1\n",
      "w i d e r _ : 1\n",
      "w i d e s t _ : 1\n"
     ]
    }
   ],
   "source": [
    "# Create initial vocabulary:\n",
    "# Each word is represented as a sequence of characters separated by spaces.\n",
    "vocab = {\" \".join(word): 1 for word in corpus}\n",
    "print(\"Initial Vocabulary:\")\n",
    "for word, freq in vocab.items():\n",
    "    print(f\"{word} : {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(vocab, allowed_tokens):\n",
    "    \"\"\"\n",
    "    Count frequency of adjacent symbol pairs in the vocabulary.\n",
    "    but only count pairs where both symbols are in allowed_tokens.\n",
    "\n",
    "    vocab: dict mapping \"words\" (as strings of space-separated symbols) to frequency.\n",
    "    allowed_tokens: a set (or list) of tokens that are allowed (e.g., sorted unique characters).\n",
    "\n",
    "    Returns a dictionary with keys as tuple (symbol1, symbol2) and values as counts.\n",
    "    \"\"\"\n",
    "    pairs = {}\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        # Count each adjacent pair in the word\n",
    "        for i in range(len(symbols) - 1):\n",
    "            if symbols[i] in allowed_tokens and symbols[i + 1] in allowed_tokens:\n",
    "                pair = (symbols[i], symbols[i + 1])\n",
    "                pairs[pair] = pairs.get(pair, 0) + freq\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pair frequencies (only counting pairs with allowed tokens):\n",
      "('l', 'o'): 3\n",
      "('o', 'w'): 3\n",
      "('w', '_'): 2\n",
      "('w', 'e'): 4\n",
      "('e', 'r'): 3\n",
      "('r', '_'): 3\n",
      "('e', 's'): 3\n",
      "('s', 't'): 3\n",
      "('t', '_'): 3\n",
      "('n', 'e'): 3\n",
      "('e', 'w'): 3\n",
      "('w', 'i'): 3\n",
      "('i', 'd'): 3\n",
      "('d', 'e'): 3\n",
      "('e', '_'): 1\n"
     ]
    }
   ],
   "source": [
    "# Perform a single example merge iteration using the modified get_stats.\n",
    "allowed_tokens = set(sorted_unique_chars)\n",
    "pairs = get_stats(vocab, allowed_tokens )\n",
    "print(\"\\nPair frequencies (only counting pairs with allowed tokens):\")\n",
    "for pair, count in pairs.items():\n",
    "    print(f\"{pair}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab(pair, vocab):\n",
    "    \"\"\"\n",
    "    Merge all occurrences of the given pair in the vocabulary.\n",
    "    pair: a tuple (symbol1, symbol2) to be merged.\n",
    "    vocab: the current vocabulary dictionary.\n",
    "    Returns a new vocabulary with the merged pair.\n",
    "    \"\"\"\n",
    "    new_vocab = {}\n",
    "    # Create the merged token\n",
    "    merged_token = ''.join(pair)\n",
    "    # Define a string pattern that represents the pair separated by a space\n",
    "    bigram = ' '.join(pair)\n",
    "    \n",
    "    for word, freq in vocab.items():\n",
    "        # Replace the pair with the merged token in the word.\n",
    "        new_word = word.replace(bigram, merged_token)\n",
    "        new_vocab[new_word] = freq\n",
    "    return new_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merging steps:\n",
      "(lo,we) ,_,d,e,i,l,lo,lowe,n,ne,o,r,r_,s,st,st_,t,w,w_,we,wi,wid,wide\n",
      "(ne,we) ,_,d,e,i,l,lo,lowe,n,ne,newe,o,r,r_,s,st,st_,t,w,w_,we,wi,wid,wide\n",
      "(lo,w_) ,_,d,e,i,l,lo,low_,lowe,n,ne,newe,o,r,r_,s,st,st_,t,w,w_,we,wi,wid,wide\n",
      "(lowe,r_) ,_,d,e,i,l,lo,low_,lowe,lower_,n,ne,newe,o,r,r_,s,st,st_,t,w,w_,we,wi,wid,wide\n",
      "(lowe,st_) ,_,d,e,i,l,lo,low_,lowe,lower_,lowest_,n,ne,newe,o,r,r_,s,st,st_,t,w,w_,we,wi,wid,wide\n",
      "(ne,w_) ,_,d,e,i,l,lo,low_,lowe,lower_,lowest_,n,ne,new_,newe,o,r,r_,s,st,st_,t,w,w_,we,wi,wid,wide\n",
      "(newe,r_) ,_,d,e,i,l,lo,low_,lowe,lower_,lowest_,n,ne,new_,newe,newer_,o,r,r_,s,st,st_,t,w,w_,we,wi,wid,wide\n",
      "(newe,st_) ,_,d,e,i,l,lo,low_,lowe,lower_,lowest_,n,ne,new_,newe,newer_,newest_,o,r,r_,s,st,st_,t,w,w_,we,wi,wid,wide\n",
      "(wide,_) ,_,d,e,i,l,lo,low_,lowe,lower_,lowest_,n,ne,new_,newe,newer_,newest_,o,r,r_,s,st,st_,t,w,w_,we,wi,wid,wide,wide_\n",
      "(wide,r_) ,_,d,e,i,l,lo,low_,lowe,lower_,lowest_,n,ne,new_,newe,newer_,newest_,o,r,r_,s,st,st_,t,w,w_,we,wi,wid,wide,wide_,wider_\n",
      "(wide,st_) ,_,d,e,i,l,lo,low_,lowe,lower_,lowest_,n,ne,new_,newe,newer_,newest_,o,r,r_,s,st,st_,t,w,w_,we,wi,wid,wide,wide_,wider_,widest_\n"
     ]
    }
   ],
   "source": [
    "# Number of merge operations\n",
    "num_merges = 15       # a parameter of BPE algo\n",
    "\n",
    "print(\"\\nMerging steps:\")\n",
    "\n",
    "# Perform iterative merging.\n",
    "for i in range(num_merges):\n",
    "\n",
    "  # Compute pair frequencies only for pairs where both tokens are allowed.\n",
    "  pairs = get_stats(vocab, allowed_tokens)\n",
    "  if not pairs:\n",
    "    break\n",
    "\n",
    "  # Select the most frequent pair.\n",
    "  best_pair = max(pairs, key=pairs.get)\n",
    "\n",
    "  vocab = merge_vocab(best_pair, vocab)\n",
    "  merged_token = ''.join(best_pair)\n",
    "  allowed_tokens.add(merged_token)\n",
    "\n",
    "    # For output, show the merged pair and the updated sorted allowed tokens.\n",
    "  output_line = f\"({best_pair[0]},{best_pair[1]}) ,\"\n",
    "  output_line += \",\".join(sorted(allowed_tokens))\n",
    "  print(output_line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Word Piece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s very similar to BPE in terms of the training, but the actual tokenization is done differently.   Like BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet. Since it identifies subwords by adding a prefix (like ## for BERT), each word is initially split by adding that prefix to all the characters inside the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, again like BPE, WordPiece learns merge rules. The main difference is the way the pair to be merged is selected. Instead of selecting the most frequent pair, WordPiece computes a score for each pair, using the following formula:  \n",
    "   score = (freq_of_pair) / (freq_of_first_element × freq_of_second_element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial global vocabulary:\n",
      "['[UNK]', 'd', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w']\n",
      "\n",
      "Starting WordPiece training...\n",
      "\n",
      "Merge 1: Merged pair ('i', 'd') -> ##id with score 1.0000\n",
      "Global vocabulary size: 12\n",
      "Current global vocabulary:\n",
      "##id, [UNK], d, e, i, l, n, o, r, s, t, w\n",
      "------------------------------------------------------------\n",
      "Merge 2: Merged pair ('l', 'o') -> lo with score 0.5000\n",
      "Global vocabulary size: 13\n",
      "Current global vocabulary:\n",
      "##id, [UNK], d, e, i, l, lo, n, o, r, s, t, w\n",
      "------------------------------------------------------------\n",
      "Merge 3: Merged pair ('s', 't') -> ##st with score 0.5000\n",
      "Global vocabulary size: 14\n",
      "Current global vocabulary:\n",
      "##id, ##st, [UNK], d, e, i, l, lo, n, o, r, s, t, w\n",
      "------------------------------------------------------------\n",
      "Merge 4: Merged pair ('lo', 'w') -> low with score 0.2500\n",
      "Global vocabulary size: 15\n",
      "Current global vocabulary:\n",
      "##id, ##st, [UNK], d, e, i, l, lo, low, n, o, r, s, t, w\n",
      "------------------------------------------------------------\n",
      "Merge 5: Merged pair ('w', '##id') -> w##id with score 0.5000\n",
      "Global vocabulary size: 16\n",
      "Current global vocabulary:\n",
      "##id, ##st, [UNK], d, e, i, l, lo, low, n, o, r, s, t, w, w##id\n",
      "------------------------------------------------------------\n",
      "Merge 6: Merged pair ('e', 'r') -> ##er with score 0.2500\n",
      "Global vocabulary size: 17\n",
      "Current global vocabulary:\n",
      "##er, ##id, ##st, [UNK], d, e, i, l, lo, low, n, o, r, s, t, w, w##id\n",
      "------------------------------------------------------------\n",
      "Merge 7: Merged pair ('low', '##er') -> low##er with score 0.5000\n",
      "Global vocabulary size: 18\n",
      "Current global vocabulary:\n",
      "##er, ##id, ##st, [UNK], d, e, i, l, lo, low, low##er, n, o, r, s, t, w, w##id\n",
      "------------------------------------------------------------\n",
      "Merge 8: Merged pair ('n', 'e') -> ne with score 0.3333\n",
      "Global vocabulary size: 19\n",
      "Current global vocabulary:\n",
      "##er, ##id, ##st, [UNK], d, e, i, l, lo, low, low##er, n, ne, o, r, s, t, w, w##id\n",
      "------------------------------------------------------------\n",
      "Merge 9: Merged pair ('ne', 'w') -> new with score 1.0000\n",
      "Global vocabulary size: 20\n",
      "Current global vocabulary:\n",
      "##er, ##id, ##st, [UNK], d, e, i, l, lo, low, low##er, n, ne, new, o, r, s, t, w, w##id\n",
      "------------------------------------------------------------\n",
      "Merge 10: Merged pair ('new', 'e') -> newe with score 0.5000\n",
      "Global vocabulary size: 21\n",
      "Current global vocabulary:\n",
      "##er, ##id, ##st, [UNK], d, e, i, l, lo, low, low##er, n, ne, new, newe, o, r, s, t, w, w##id\n",
      "------------------------------------------------------------\n",
      "Merge 11: Merged pair ('w##id', 'e') -> w##ide with score 1.0000\n",
      "Global vocabulary size: 22\n",
      "Current global vocabulary:\n",
      "##er, ##id, ##st, [UNK], d, e, i, l, lo, low, low##er, n, ne, new, newe, o, r, s, t, w, w##id, w##ide\n",
      "------------------------------------------------------------\n",
      "Merge 12: Merged pair ('newe', '##st') -> newe##st with score 0.5000\n",
      "Global vocabulary size: 23\n",
      "Current global vocabulary:\n",
      "##er, ##id, ##st, [UNK], d, e, i, l, lo, low, low##er, n, ne, new, newe, newe##st, o, r, s, t, w, w##id, w##ide\n",
      "------------------------------------------------------------\n",
      "Merge 13: Merged pair ('w##ide', '##st') -> w##ide##st with score 1.0000\n",
      "Global vocabulary size: 24\n",
      "Current global vocabulary:\n",
      "##er, ##id, ##st, [UNK], d, e, i, l, lo, low, low##er, n, ne, new, newe, newe##st, o, r, s, t, w, w##id, w##ide, w##ide##st\n",
      "------------------------------------------------------------\n",
      "\n",
      "Final global vocabulary:\n",
      "##er, ##id, ##st, [UNK], d, e, i, l, lo, low, low##er, n, ne, new, newe, newe##st, o, r, s, t, w, w##id, w##ide, w##ide##st\n"
     ]
    }
   ],
   "source": [
    "def initialize_corpus(corpus):\n",
    "    \"\"\"\n",
    "    Given a list of words, tokenize each word into a list of individual characters.\n",
    "    Returns:\n",
    "        vocab: a list of token lists, one per word.\n",
    "        global_vocab: a set containing all tokens (initial characters) plus [UNK].\n",
    "    \"\"\"\n",
    "    tokenized_corpus = []\n",
    "    global_vocab = set()\n",
    "    for word in corpus:\n",
    "        tokens = list(word)\n",
    "        tokenized_corpus.append(tokens)\n",
    "        global_vocab.update(tokens)\n",
    "    global_vocab.add(\"[UNK]\")  # Add unknown token\n",
    "    return tokenized_corpus, global_vocab\n",
    "\n",
    "def get_token_freq(tokenized_corpus):\n",
    "    \"\"\"\n",
    "    Count frequencies of individual tokens in the current corpus.\n",
    "    \"\"\"\n",
    "    freq = {}\n",
    "    for tokens in tokenized_corpus:\n",
    "        for token in tokens:\n",
    "            freq[token] = freq.get(token, 0) + 1\n",
    "    return freq\n",
    "\n",
    "def get_pair_freq(tokenized_corpus):\n",
    "    \"\"\"\n",
    "    Count frequencies of adjacent token pairs in the corpus.\n",
    "    Returns:\n",
    "        A dictionary mapping a tuple (token1, token2) to its frequency.\n",
    "    \"\"\"\n",
    "    pair_freq = {}\n",
    "    for tokens in tokenized_corpus:\n",
    "        for i in range(len(tokens) - 1):\n",
    "            pair = (tokens[i], tokens[i+1])\n",
    "            pair_freq[pair] = pair_freq.get(pair, 0) + 1\n",
    "    return pair_freq\n",
    "\n",
    "def select_best_pair(pair_freq, token_freq):\n",
    "    \"\"\"\n",
    "    Evaluate candidate merges using the scoring function:\n",
    "      score = P(xy) / (P(x)*P(y))\n",
    "    Returns the best pair (tuple) and its score.\n",
    "    \"\"\"\n",
    "    best_pair = None\n",
    "    best_score = -1.0\n",
    "    for pair, freq in pair_freq.items():\n",
    "        # Use frequencies as proxies for probability counts.\n",
    "        p_x = token_freq.get(pair[0], 1)\n",
    "        p_y = token_freq.get(pair[1], 1)\n",
    "        score = freq / (p_x * p_y)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_pair = pair\n",
    "    return best_pair, best_score\n",
    "\n",
    "def merge_corpus(best_pair, tokenized_corpus):\n",
    "    \"\"\"\n",
    "    Merge the best pair in every word in the corpus.\n",
    "    For each occurrence of the pair:\n",
    "       - if the pair occurs at the beginning of the word, merge without a prefix;\n",
    "       - otherwise, add a \"##\" prefix to the merged token.\n",
    "    Returns the updated corpus and a set of newly created merged tokens.\n",
    "    \"\"\"\n",
    "    new_tokens_set = set()\n",
    "    merged_token_plain = \"\".join(best_pair)  # unprefixed version\n",
    "    new_corpus = []\n",
    "    \n",
    "    for tokens in tokenized_corpus:\n",
    "        new_tokens = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            # Check if we can merge the next two tokens.\n",
    "            if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == best_pair:\n",
    "                # If this merge is at the start of the word, do not prefix.\n",
    "                if i == 0:\n",
    "                    new_token = merged_token_plain\n",
    "                else:\n",
    "                    new_token = \"##\" + merged_token_plain\n",
    "                new_tokens.append(new_token)\n",
    "                new_tokens_set.add(new_token)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "        new_corpus.append(new_tokens)\n",
    "    return new_corpus, new_tokens_set\n",
    "\n",
    "# -------------------------------\n",
    "# Main WordPiece Training Procedure\n",
    "# -------------------------------\n",
    "\n",
    "# Sample training corpus (list of words)\n",
    "corpus = [\"low\", \"lower\", \"newest\", \"widest\"]\n",
    "\n",
    "# Initialize the corpus and global vocabulary (starting with individual characters)\n",
    "tokenized_corpus, global_vocab = initialize_corpus(corpus)\n",
    "\n",
    "# Set training parameters:\n",
    "max_vocab_size = 50        # desired maximum vocabulary size\n",
    "score_threshold = 0.1      # minimum score threshold to accept a merge\n",
    "max_merges = 100           # maximum number of merge iterations\n",
    "\n",
    "print(\"Initial global vocabulary:\")\n",
    "print(sorted(global_vocab))\n",
    "print(\"\\nStarting WordPiece training...\\n\")\n",
    "\n",
    "for merge_iter in range(max_merges):\n",
    "    token_freq = get_token_freq(tokenized_corpus)\n",
    "    pair_freq = get_pair_freq(tokenized_corpus)\n",
    "    \n",
    "    if not pair_freq:\n",
    "        break\n",
    "    \n",
    "    best_pair, best_score = select_best_pair(pair_freq, token_freq)\n",
    "    \n",
    "    # Stop if the best score is below the threshold\n",
    "    if best_score < score_threshold:\n",
    "        print(f\"Stopping: best merge score {best_score:.4f} fell below the threshold {score_threshold}.\")\n",
    "        break\n",
    "    \n",
    "    # Stop if the global vocabulary has reached the desired size.\n",
    "    if len(global_vocab) >= max_vocab_size:\n",
    "        print(f\"Stopping: reached maximum vocabulary size of {max_vocab_size}.\")\n",
    "        break\n",
    "    \n",
    "    # Merge the best pair in the corpus.\n",
    "    tokenized_corpus, new_tokens = merge_corpus(best_pair, tokenized_corpus)\n",
    "    \n",
    "    # Update the global vocabulary with the new token(s).\n",
    "    global_vocab.update(new_tokens)\n",
    "    \n",
    "    print(f\"Merge {merge_iter+1}: Merged pair {best_pair} ->\", end=\" \")\n",
    "    # Print merged tokens (could be both with and without prefix depending on position)\n",
    "    print(\", \".join(sorted(new_tokens)), f\"with score {best_score:.4f}\")\n",
    "    print(\"Global vocabulary size:\", len(global_vocab))\n",
    "    print(\"Current global vocabulary:\")\n",
    "    print(\", \".join(sorted(global_vocab)))\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nFinal global vocabulary:\")\n",
    "print(\", \".join(sorted(global_vocab)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flow of Word Piece:  \n",
    "\n",
    "<img src=\"C:\\Shilpi\\BFH\\MSE_Courses\\Semester3\\TSM_AdvNLP\\Lec1_Intro\\Word Piece.png\" alt=\"Sample Image\" width=\"150\"> \n",
    " \n",
    "https://medium.com/@atharv6f_47401/wordpiece-tokenization-a-bpe-variant-73cc48865cbf\n",
    "\n",
    "1. I/P coupus -> initialize vocab with charac -> Add special tikens UNK, ## -> split corpus into words -> generate all possible subword combinatons -> calculate token frequencies -> Iterative merging process   \n",
    "2. check if vocab size is reached or Score below threshold  \n",
    "3. If yes, finalise vocab ->tokenize of new Text -> split into words -> for each word -> match longest subword from Start -> if match found..  \n",
    "yes -> add to o/p -> check for more characters -> (if yes -> again match longest subword from Start, and continue again.) (if no, check for more words -> )  \n",
    "no -> add UNK token -> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Differences between BPE and WordPiece  \n",
    "Initialization  \n",
    "BPE: Initializes the vocabulary with individual characters or bytes.  \n",
    "WordPiece: Starts with characters plus special tokens, specifically UNK (unknown) and HASH (representing ##) for non-initial subwords.  \n",
    "\n",
    "Subword Generation  \n",
    "BPE: Directly counts the frequencies of character pairs in the corpus.  \n",
    "WordPiece: Generates all possible subword combinations before scoring.  \n",
    "\n",
    "Merging Criterion    \n",
    "BPE: Selects and merges the most frequent pair in each iteration.  \n",
    "WordPiece: Calculates scores for pairs and selects the highest scoring pair for merging.  \n",
    "\n",
    "Special Token Usage  \n",
    "BPE: Does not use special tokens in its base algorithm.  \n",
    "WordPiece: Utilizes the HASH prefix (##) for non-word-initial subwords, maintaining word boundary information.  \n",
    "\n",
    "Stopping Criterion  \n",
    "BPE: Typically stops when a predefined vocabulary size is reached.  \n",
    "WordPiece: Can stop based on vocabulary size or when scores fall below a certain threshold.  \n",
    "\n",
    "Tokenization of New Text  \n",
    "BPE: Applies learned merge rules sequentially.  \n",
    "WordPiece: Uses a longest match approach, incorporating UNK for unknown words and HASH for non-initial subwords.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Hugging Face Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section5.ipynb   \n",
    "\n",
    "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section6.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets evaluate transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\itssh\\anaconda\\envs\\AdvNLP\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'low': 1,\n",
       "             '_': 9,\n",
       "             'lower': 1,\n",
       "             'lowest': 1,\n",
       "             'new': 1,\n",
       "             'newer': 1,\n",
       "             'newest': 1,\n",
       "             'wide': 1,\n",
       "             'wider': 1,\n",
       "             'widest': 1})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# creating default dictionary of corpus generated at the start of this notebook\n",
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "word_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['##d', '##e', '##i', '##o', '##r', '##s', '##t', '##w', '_', 'l', 'n', 'w']\n"
     ]
    }
   ],
   "source": [
    "# generating characters with adding suffix ##\n",
    "alphabet = []\n",
    "for word in word_freqs.keys():\n",
    "    if word[0] not in alphabet:\n",
    "        alphabet.append(word[0])\n",
    "    for letter in word[1:]:\n",
    "        if f\"##{letter}\" not in alphabet:\n",
    "            alphabet.append(f\"##{letter}\")\n",
    "\n",
    "alphabet.sort()\n",
    "alphabet\n",
    "\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding an unkown token to the  characters list\n",
    "vocab = [\"[UNK]\"] + alphabet.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to tokenize each word into subword units\n",
    "splits = {\n",
    "    word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
    "    for word in word_freqs.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute pair scores\n",
    "def compute_pair_scores(splits):\n",
    "    letter_freqs = defaultdict(int)\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            letter_freqs[split[0]] += freq\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            letter_freqs[split[i]] += freq\n",
    "            pair_freqs[pair] += freq\n",
    "        letter_freqs[split[-1]] += freq\n",
    "\n",
    "    scores = {\n",
    "        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
    "        for pair, freq in pair_freqs.items()\n",
    "    }\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('l', '##o'): 0.3333333333333333\n",
      "('##o', '##w'): 0.16666666666666666\n",
      "('##w', '##e'): 0.06666666666666667\n",
      "('##e', '##r'): 0.1\n",
      "('##e', '##s'): 0.1\n",
      "('##s', '##t'): 0.3333333333333333\n",
      "('n', '##e'): 0.1\n",
      "('##e', '##w'): 0.05\n",
      "('w', '##i'): 0.3333333333333333\n",
      "('##i', '##d'): 0.3333333333333333\n",
      "('##d', '##e'): 0.1\n"
     ]
    }
   ],
   "source": [
    "# calculating pair scores\n",
    "pair_scores = compute_pair_scores(splits)\n",
    "for i, key in enumerate(pair_scores.keys()):\n",
    "    print(f\"{key}: {pair_scores[key]}\")\n",
    "    if i >= 15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('l', '##o') 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# checking the best pair score\n",
    "best_pair = \"\"\n",
    "max_score = None\n",
    "for pair, score in pair_scores.items():\n",
    "    if max_score is None or max_score < score:\n",
    "        best_pair = pair\n",
    "        max_score = score\n",
    "\n",
    "print(best_pair, max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using merge_pair defined here.\n",
      "merge_pair was called successfully.\n"
     ]
    }
   ],
   "source": [
    "# merging pairs as per score order\n",
    "print(\"Using merge_pair defined here.\")\n",
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
    "                split = split[:i] + [merge] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits\n",
    "print(\"merge_pair was called successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_pair: ('l', '##o'), type: <class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "# checking if best_pair is tuple\n",
    "print(f\"best_pair: {best_pair}, type: {type(best_pair)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'low': ['low'],\n",
       " '_': ['_'],\n",
       " 'lower': ['lower'],\n",
       " 'lowest': ['lowest'],\n",
       " 'new': ['new'],\n",
       " 'newer': ['newer'],\n",
       " 'newest': ['newest'],\n",
       " 'wide': ['wide'],\n",
       " 'wider': ['wider'],\n",
       " 'widest': ['widest']}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_pair('l', '##o', splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'startswith'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m         max_score \u001b[38;5;241m=\u001b[39m score\n\u001b[0;32m     12\u001b[0m splits \u001b[38;5;241m=\u001b[39m merge_pair(\u001b[38;5;241m*\u001b[39mbest_pair, splits) \n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mbest_pair\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartswith\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m##\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(best_pair[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m     15\u001b[0m     new_token \u001b[38;5;241m=\u001b[39m best_pair[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m best_pair[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m2\u001b[39m:]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'startswith'"
     ]
    }
   ],
   "source": [
    "# running and generating the merged pairs\n",
    "vocab_size = 70\n",
    "while len(vocab) < vocab_size:\n",
    "    scores = compute_pair_scores(splits)\n",
    "\n",
    "    best_pair, max_score = (None, None), None\n",
    "    for pair, score in scores.items():\n",
    "        if max_score is None or max_score < score:\n",
    "            best_pair = pair\n",
    "            max_score = score\n",
    "\n",
    "    splits = merge_pair(*best_pair, splits) \n",
    "\n",
    "    if best_pair[1].startswith(\"##\") and len(best_pair[1]) > 2:\n",
    "        new_token = best_pair[0] + best_pair[1][2:]\n",
    "    else:\n",
    "        new_token = best_pair[0] + best_pair[1]\n",
    "\n",
    "    vocab.append(new_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '##d', '##e', '##i', '##o', '##r', '##s', '##t', '##w', '_', 'l', 'n', 'w', 'lo', '##st', 'wi', 'wid', 'low', '##er', '##est', 'ne', 'new', 'wide', 'wider', 'widest', 'lower', 'newer', 'lowest', 'newest']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: I have issue with merge_pair functon which is not clearly generating me the pairs. Please guide me to the issue happeining here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AdvNLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
