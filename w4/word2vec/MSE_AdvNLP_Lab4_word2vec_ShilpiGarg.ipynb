{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "quick-polls",
   "metadata": {},
   "source": [
    "![MSE Logo](https://moodle.msengineering.ch/pluginfile.php/1/core_admin/logo/0x150/1643104191/logo-mse.png)\n",
    "\n",
    "# AdvNLP Lab 4 GRADED: Testing a pretrained word2vec model on analogy tasks\n",
    "\n",
    "**Objectives:**  experiment with *word vectors* from word2vec: test them on analogy tasks; use *accuracy and MRR* (Mean Reciprocal Rank) scores.\n",
    "\n",
    "**Useful documentation:** the [section on KeyedVectors in Gensim](https://radimrehurek.com/gensim/models/keyedvectors.html) and possibly the [section on word2vec](https://radimrehurek.com/gensim/models/word2vec.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-chase",
   "metadata": {},
   "source": [
    "## 1. Word2vec model trained on Google News\n",
    "**1a.** Please install the latest version of Gensim, preferably in a Conda environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d250d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "extreme-birthday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: gensim\n",
      "Version: 4.3.3\n",
      "Summary: Python framework for fast Vector Space Modelling\n",
      "Home-page: https://radimrehurek.com/gensim/\n",
      "Author: Radim Rehurek\n",
      "Author-email: me@radimrehurek.com\n",
      "License: LGPL-2.1-only\n",
      "Location: /home/shilpi/Documents/sem3/Adv_NLP/.venv/lib/python3.12/site-packages\n",
      "Requires: numpy, scipy, smart-open\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade gensim\n",
    "# You can run the following verification:\n",
    "!pip show gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, os, random\n",
    "from gensim import downloader\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import utils\n",
    "# help(gensim.models.word2vec) # take a look if needed\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-titanium",
   "metadata": {},
   "source": [
    "**1b.** Please download from Gensim the `word2vec-google-news-300` model, upon your first use.  Then, please write code to answer the following questions:\n",
    "* Where is the model stored on your computer and what is the file name?  You can store the absolute path in a variable called `path_to_model_file`.\n",
    "* What is the size of the corresponding file?  Please display the size in gigabytes with two decimals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d71ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5428fb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Name: NVIDIA TITAN RTX\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Optionally, if a GPU is available, print its name:\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "infectious-burner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model from Gensim (needed only the first time)\n",
    "# gensim.downloader.load(\"word2vec-google-news-300\")\n",
    "# No need to store the returned value (uses a lot of memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "scheduled-binary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "model = gensim.downloader.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "756ec99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the path in variable\n",
    "path_to_model_file = '/home/shilpi/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2cf67faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1743563840\n"
     ]
    }
   ],
   "source": [
    "# Get the file size in bytes\n",
    "file_size_bytes  = os.path.getsize(path_to_model_file)\n",
    "print(file_size_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0574040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file size: 1.62 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert bytes to gigabytes (1 GB = 1024^3 bytes)\n",
    "file_size_gb = file_size_bytes / (1024 ** 3)\n",
    "\n",
    "# Print the file size in gigabytes with two decimal places\n",
    "print(f\"Model file size: {file_size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc1f368",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "western-insurance",
   "metadata": {},
   "source": [
    "**1c.** Please load the word2vec model as an instance of the class `KeyedVectors`, and store it in a variable called `wv_model`. \n",
    "What is, at this point, the memory size of the process corresponding to this notebook?  Simply write the value you obtain from any OS-specific utility that you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-jewelry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Memory used by this notebook: 11.59 GB\n"
     ]
    }
   ],
   "source": [
    "# Please write your Python code below and execute it.  Write the memory size on a commented line.\n",
    "# Load the Word2Vec model\n",
    "wv_model = KeyedVectors.load_word2vec_format(\n",
    "    path_to_model_file, \n",
    "    binary=True\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "process = psutil.Process(os.getpid())  # Get current process info\n",
    "memory_usage_gb = process.memory_info().rss / (1024 ** 3)  # Convert bytes to GB\n",
    "\n",
    "print(f\"Memory used by this notebook: {memory_usage_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-flooring",
   "metadata": {},
   "source": [
    "**1d.** Please write the instructions that generate the answers to the following questions.\n",
    "* What is the size of the vocabulary of the `wv_model` model?  \n",
    "* What is the dimensionality of each word vector?  \n",
    "* What is the word corresponding to the vector in position 1234?  \n",
    "* What are the first 10 coefficients of the word vector for the word *pyramid*?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "rubber-richardson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3000000\n"
     ]
    }
   ],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "# size of the vocabulary of the `wv_model` model\n",
    "\n",
    "vocab_size = len(wv_model)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a6cc232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality of each word vector: 300\n"
     ]
    }
   ],
   "source": [
    "# the dimensionality of each word vector\n",
    "\n",
    "vector_dim = wv_model.vector_size\n",
    "print(f\"Dimensionality of each word vector: {vector_dim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4911a159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word at position 1234: learn\n"
     ]
    }
   ],
   "source": [
    "# the word corresponding to the vector in position 1234\n",
    "word_at_1234 = wv_model.index_to_key[1234]\n",
    "print(f\"Word at position 1234: {word_at_1234}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7301759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 coefficients of the word vector for 'pyramid':\n",
      "[ 0.00402832 -0.00260925  0.04296875  0.19433594 -0.03979492 -0.06445312\n",
      "  0.42773438 -0.18359375 -0.27148438 -0.12890625]\n"
     ]
    }
   ],
   "source": [
    "# the first 10 coefficients of the word vector for the word *pyramid*\n",
    "\n",
    "word = \"pyramid\"\n",
    "\n",
    "if word in wv_model:\n",
    "    vector = wv_model[word]  # Get the word vector\n",
    "    first_10_coefficients = vector[:10]  # Extract the first 10 coefficients\n",
    "    print(f\"First 10 coefficients of the word vector for '{word}':\\n{first_10_coefficients}\")\n",
    "else:\n",
    "    print(f\"'{word}' is not in the vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-accessory",
   "metadata": {},
   "source": [
    "## 2. Solving analogies using word2vec trained on Google News\n",
    "In this section, you are going to use word vectors to solve analogy tasks provided with Gensim, such as \"What is to France what Rome is to Italy?\".  The predefined function in Gensim that evaluates a model on this task does not provide enough details, so you will need to make modifications to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a642353b",
   "metadata": {},
   "source": [
    "**2a.** The analogy tasks are stored in a text file called `questions-words.txt` which is typically found in `C:\\Users\\YourNameHere\\.conda\\envs\\YourEnvNameHere\\Lib\\site-packages\\gensim\\test\\test_data`.  You can access it from here with Gensim as `datapath('questions-words.txt')`.  \n",
    "\n",
    "Please create a file called `questions-words-100.txt` with the first 100 lines from the original file.  Please run the evaluation task on this file, using the [documentation of the KeyedVectors class](https://radimrehurek.com/gensim/models/keyedvectors.html), then answer the following questions:\n",
    "* How many analogy tasks are there in your `questions-words-100.txt` file?\n",
    "* How many analogies were solved correctly and how many incorrectly?\n",
    "* What is the accuracy returned by `evaluate_word_analogies`?\n",
    "* How much time did it take to solve the analogies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ae43e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 100 lines saved to: questions_words-100.txt\n"
     ]
    }
   ],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "# to save first 100 line in other file\n",
    "\n",
    "# Define file paths\n",
    "input_file = \"/home/shilpi/Documents/sem3/Adv_NLP/.venv/lib/python3.12/site-packages/gensim/test/test_data/questions-words.txt\"\n",
    "output_file = \"questions_words-100.txt\"  # New file to save the output\n",
    "\n",
    "# Read and save the first 100 lines\n",
    "with open(input_file, \"r\") as infile, open(output_file, \"w\") as outfile:\n",
    "    for _ in range(100):\n",
    "        line = infile.readline()\n",
    "        if not line:\n",
    "            break  # Stop if there are fewer than 100 lines\n",
    "        outfile.write(line)  # Write the line to the new file\n",
    "\n",
    "print(f\"First 100 lines saved to: {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d3ff4e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8080808080808081"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluations task \n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "analogy_scores, sections = wv_model.evaluate_word_analogies('questions_words-100.txt')\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# accuracy returened by evalute_word_analogies\n",
    "analogy_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cf5b373a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'section': 'capital-common-countries',\n",
       "  'correct': [('ATHENS', 'GREECE', 'BANGKOK', 'THAILAND'),\n",
       "   ('ATHENS', 'GREECE', 'BEIJING', 'CHINA'),\n",
       "   ('ATHENS', 'GREECE', 'BERLIN', 'GERMANY'),\n",
       "   ('ATHENS', 'GREECE', 'BERN', 'SWITZERLAND'),\n",
       "   ('ATHENS', 'GREECE', 'CAIRO', 'EGYPT'),\n",
       "   ('ATHENS', 'GREECE', 'CANBERRA', 'AUSTRALIA'),\n",
       "   ('ATHENS', 'GREECE', 'HAVANA', 'CUBA'),\n",
       "   ('ATHENS', 'GREECE', 'HELSINKI', 'FINLAND'),\n",
       "   ('ATHENS', 'GREECE', 'ISLAMABAD', 'PAKISTAN'),\n",
       "   ('ATHENS', 'GREECE', 'MADRID', 'SPAIN'),\n",
       "   ('ATHENS', 'GREECE', 'MOSCOW', 'RUSSIA'),\n",
       "   ('ATHENS', 'GREECE', 'OSLO', 'NORWAY'),\n",
       "   ('ATHENS', 'GREECE', 'OTTAWA', 'CANADA'),\n",
       "   ('ATHENS', 'GREECE', 'PARIS', 'FRANCE'),\n",
       "   ('ATHENS', 'GREECE', 'ROME', 'ITALY'),\n",
       "   ('ATHENS', 'GREECE', 'STOCKHOLM', 'SWEDEN'),\n",
       "   ('ATHENS', 'GREECE', 'TEHRAN', 'IRAN'),\n",
       "   ('ATHENS', 'GREECE', 'TOKYO', 'JAPAN'),\n",
       "   ('BAGHDAD', 'IRAQ', 'BANGKOK', 'THAILAND'),\n",
       "   ('BAGHDAD', 'IRAQ', 'BEIJING', 'CHINA'),\n",
       "   ('BAGHDAD', 'IRAQ', 'BERLIN', 'GERMANY'),\n",
       "   ('BAGHDAD', 'IRAQ', 'CAIRO', 'EGYPT'),\n",
       "   ('BAGHDAD', 'IRAQ', 'HANOI', 'VIETNAM'),\n",
       "   ('BAGHDAD', 'IRAQ', 'HAVANA', 'CUBA'),\n",
       "   ('BAGHDAD', 'IRAQ', 'HELSINKI', 'FINLAND'),\n",
       "   ('BAGHDAD', 'IRAQ', 'ISLAMABAD', 'PAKISTAN'),\n",
       "   ('BAGHDAD', 'IRAQ', 'KABUL', 'AFGHANISTAN'),\n",
       "   ('BAGHDAD', 'IRAQ', 'MADRID', 'SPAIN'),\n",
       "   ('BAGHDAD', 'IRAQ', 'MOSCOW', 'RUSSIA'),\n",
       "   ('BAGHDAD', 'IRAQ', 'OSLO', 'NORWAY'),\n",
       "   ('BAGHDAD', 'IRAQ', 'PARIS', 'FRANCE'),\n",
       "   ('BAGHDAD', 'IRAQ', 'ROME', 'ITALY'),\n",
       "   ('BAGHDAD', 'IRAQ', 'STOCKHOLM', 'SWEDEN'),\n",
       "   ('BAGHDAD', 'IRAQ', 'TEHRAN', 'IRAN'),\n",
       "   ('BAGHDAD', 'IRAQ', 'TOKYO', 'JAPAN'),\n",
       "   ('BAGHDAD', 'IRAQ', 'ATHENS', 'GREECE'),\n",
       "   ('BANGKOK', 'THAILAND', 'BEIJING', 'CHINA'),\n",
       "   ('BANGKOK', 'THAILAND', 'BERLIN', 'GERMANY'),\n",
       "   ('BANGKOK', 'THAILAND', 'BERN', 'SWITZERLAND'),\n",
       "   ('BANGKOK', 'THAILAND', 'CAIRO', 'EGYPT'),\n",
       "   ('BANGKOK', 'THAILAND', 'CANBERRA', 'AUSTRALIA'),\n",
       "   ('BANGKOK', 'THAILAND', 'HAVANA', 'CUBA'),\n",
       "   ('BANGKOK', 'THAILAND', 'HELSINKI', 'FINLAND'),\n",
       "   ('BANGKOK', 'THAILAND', 'ISLAMABAD', 'PAKISTAN'),\n",
       "   ('BANGKOK', 'THAILAND', 'KABUL', 'AFGHANISTAN'),\n",
       "   ('BANGKOK', 'THAILAND', 'MADRID', 'SPAIN'),\n",
       "   ('BANGKOK', 'THAILAND', 'MOSCOW', 'RUSSIA'),\n",
       "   ('BANGKOK', 'THAILAND', 'OSLO', 'NORWAY'),\n",
       "   ('BANGKOK', 'THAILAND', 'OTTAWA', 'CANADA'),\n",
       "   ('BANGKOK', 'THAILAND', 'PARIS', 'FRANCE'),\n",
       "   ('BANGKOK', 'THAILAND', 'ROME', 'ITALY'),\n",
       "   ('BANGKOK', 'THAILAND', 'STOCKHOLM', 'SWEDEN'),\n",
       "   ('BANGKOK', 'THAILAND', 'TEHRAN', 'IRAN'),\n",
       "   ('BANGKOK', 'THAILAND', 'TOKYO', 'JAPAN'),\n",
       "   ('BANGKOK', 'THAILAND', 'ATHENS', 'GREECE'),\n",
       "   ('BEIJING', 'CHINA', 'BERLIN', 'GERMANY'),\n",
       "   ('BEIJING', 'CHINA', 'CAIRO', 'EGYPT'),\n",
       "   ('BEIJING', 'CHINA', 'CANBERRA', 'AUSTRALIA'),\n",
       "   ('BEIJING', 'CHINA', 'HAVANA', 'CUBA'),\n",
       "   ('BEIJING', 'CHINA', 'HELSINKI', 'FINLAND'),\n",
       "   ('BEIJING', 'CHINA', 'ISLAMABAD', 'PAKISTAN'),\n",
       "   ('BEIJING', 'CHINA', 'MADRID', 'SPAIN'),\n",
       "   ('BEIJING', 'CHINA', 'MOSCOW', 'RUSSIA'),\n",
       "   ('BEIJING', 'CHINA', 'OSLO', 'NORWAY'),\n",
       "   ('BEIJING', 'CHINA', 'OTTAWA', 'CANADA'),\n",
       "   ('BEIJING', 'CHINA', 'PARIS', 'FRANCE'),\n",
       "   ('BEIJING', 'CHINA', 'ROME', 'ITALY'),\n",
       "   ('BEIJING', 'CHINA', 'STOCKHOLM', 'SWEDEN'),\n",
       "   ('BEIJING', 'CHINA', 'TEHRAN', 'IRAN'),\n",
       "   ('BEIJING', 'CHINA', 'TOKYO', 'JAPAN'),\n",
       "   ('BEIJING', 'CHINA', 'ATHENS', 'GREECE'),\n",
       "   ('BEIJING', 'CHINA', 'BANGKOK', 'THAILAND'),\n",
       "   ('BERLIN', 'GERMANY', 'BERN', 'SWITZERLAND'),\n",
       "   ('BERLIN', 'GERMANY', 'CAIRO', 'EGYPT'),\n",
       "   ('BERLIN', 'GERMANY', 'CANBERRA', 'AUSTRALIA'),\n",
       "   ('BERLIN', 'GERMANY', 'HAVANA', 'CUBA'),\n",
       "   ('BERLIN', 'GERMANY', 'HELSINKI', 'FINLAND'),\n",
       "   ('BERLIN', 'GERMANY', 'ISLAMABAD', 'PAKISTAN'),\n",
       "   ('BERLIN', 'GERMANY', 'MADRID', 'SPAIN'),\n",
       "   ('BERLIN', 'GERMANY', 'MOSCOW', 'RUSSIA')],\n",
       "  'incorrect': [('ATHENS', 'GREECE', 'BAGHDAD', 'IRAQ'),\n",
       "   ('ATHENS', 'GREECE', 'HANOI', 'VIETNAM'),\n",
       "   ('ATHENS', 'GREECE', 'KABUL', 'AFGHANISTAN'),\n",
       "   ('ATHENS', 'GREECE', 'LONDON', 'ENGLAND'),\n",
       "   ('BAGHDAD', 'IRAQ', 'BERN', 'SWITZERLAND'),\n",
       "   ('BAGHDAD', 'IRAQ', 'CANBERRA', 'AUSTRALIA'),\n",
       "   ('BAGHDAD', 'IRAQ', 'LONDON', 'ENGLAND'),\n",
       "   ('BAGHDAD', 'IRAQ', 'OTTAWA', 'CANADA'),\n",
       "   ('BANGKOK', 'THAILAND', 'HANOI', 'VIETNAM'),\n",
       "   ('BANGKOK', 'THAILAND', 'LONDON', 'ENGLAND'),\n",
       "   ('BANGKOK', 'THAILAND', 'BAGHDAD', 'IRAQ'),\n",
       "   ('BEIJING', 'CHINA', 'BERN', 'SWITZERLAND'),\n",
       "   ('BEIJING', 'CHINA', 'HANOI', 'VIETNAM'),\n",
       "   ('BEIJING', 'CHINA', 'KABUL', 'AFGHANISTAN'),\n",
       "   ('BEIJING', 'CHINA', 'LONDON', 'ENGLAND'),\n",
       "   ('BEIJING', 'CHINA', 'BAGHDAD', 'IRAQ'),\n",
       "   ('BERLIN', 'GERMANY', 'HANOI', 'VIETNAM'),\n",
       "   ('BERLIN', 'GERMANY', 'KABUL', 'AFGHANISTAN'),\n",
       "   ('BERLIN', 'GERMANY', 'LONDON', 'ENGLAND')]},\n",
       " {'section': 'Total accuracy',\n",
       "  'correct': [('ATHENS', 'GREECE', 'BANGKOK', 'THAILAND'),\n",
       "   ('ATHENS', 'GREECE', 'BEIJING', 'CHINA'),\n",
       "   ('ATHENS', 'GREECE', 'BERLIN', 'GERMANY'),\n",
       "   ('ATHENS', 'GREECE', 'BERN', 'SWITZERLAND'),\n",
       "   ('ATHENS', 'GREECE', 'CAIRO', 'EGYPT'),\n",
       "   ('ATHENS', 'GREECE', 'CANBERRA', 'AUSTRALIA'),\n",
       "   ('ATHENS', 'GREECE', 'HAVANA', 'CUBA'),\n",
       "   ('ATHENS', 'GREECE', 'HELSINKI', 'FINLAND'),\n",
       "   ('ATHENS', 'GREECE', 'ISLAMABAD', 'PAKISTAN'),\n",
       "   ('ATHENS', 'GREECE', 'MADRID', 'SPAIN'),\n",
       "   ('ATHENS', 'GREECE', 'MOSCOW', 'RUSSIA'),\n",
       "   ('ATHENS', 'GREECE', 'OSLO', 'NORWAY'),\n",
       "   ('ATHENS', 'GREECE', 'OTTAWA', 'CANADA'),\n",
       "   ('ATHENS', 'GREECE', 'PARIS', 'FRANCE'),\n",
       "   ('ATHENS', 'GREECE', 'ROME', 'ITALY'),\n",
       "   ('ATHENS', 'GREECE', 'STOCKHOLM', 'SWEDEN'),\n",
       "   ('ATHENS', 'GREECE', 'TEHRAN', 'IRAN'),\n",
       "   ('ATHENS', 'GREECE', 'TOKYO', 'JAPAN'),\n",
       "   ('BAGHDAD', 'IRAQ', 'BANGKOK', 'THAILAND'),\n",
       "   ('BAGHDAD', 'IRAQ', 'BEIJING', 'CHINA'),\n",
       "   ('BAGHDAD', 'IRAQ', 'BERLIN', 'GERMANY'),\n",
       "   ('BAGHDAD', 'IRAQ', 'CAIRO', 'EGYPT'),\n",
       "   ('BAGHDAD', 'IRAQ', 'HANOI', 'VIETNAM'),\n",
       "   ('BAGHDAD', 'IRAQ', 'HAVANA', 'CUBA'),\n",
       "   ('BAGHDAD', 'IRAQ', 'HELSINKI', 'FINLAND'),\n",
       "   ('BAGHDAD', 'IRAQ', 'ISLAMABAD', 'PAKISTAN'),\n",
       "   ('BAGHDAD', 'IRAQ', 'KABUL', 'AFGHANISTAN'),\n",
       "   ('BAGHDAD', 'IRAQ', 'MADRID', 'SPAIN'),\n",
       "   ('BAGHDAD', 'IRAQ', 'MOSCOW', 'RUSSIA'),\n",
       "   ('BAGHDAD', 'IRAQ', 'OSLO', 'NORWAY'),\n",
       "   ('BAGHDAD', 'IRAQ', 'PARIS', 'FRANCE'),\n",
       "   ('BAGHDAD', 'IRAQ', 'ROME', 'ITALY'),\n",
       "   ('BAGHDAD', 'IRAQ', 'STOCKHOLM', 'SWEDEN'),\n",
       "   ('BAGHDAD', 'IRAQ', 'TEHRAN', 'IRAN'),\n",
       "   ('BAGHDAD', 'IRAQ', 'TOKYO', 'JAPAN'),\n",
       "   ('BAGHDAD', 'IRAQ', 'ATHENS', 'GREECE'),\n",
       "   ('BANGKOK', 'THAILAND', 'BEIJING', 'CHINA'),\n",
       "   ('BANGKOK', 'THAILAND', 'BERLIN', 'GERMANY'),\n",
       "   ('BANGKOK', 'THAILAND', 'BERN', 'SWITZERLAND'),\n",
       "   ('BANGKOK', 'THAILAND', 'CAIRO', 'EGYPT'),\n",
       "   ('BANGKOK', 'THAILAND', 'CANBERRA', 'AUSTRALIA'),\n",
       "   ('BANGKOK', 'THAILAND', 'HAVANA', 'CUBA'),\n",
       "   ('BANGKOK', 'THAILAND', 'HELSINKI', 'FINLAND'),\n",
       "   ('BANGKOK', 'THAILAND', 'ISLAMABAD', 'PAKISTAN'),\n",
       "   ('BANGKOK', 'THAILAND', 'KABUL', 'AFGHANISTAN'),\n",
       "   ('BANGKOK', 'THAILAND', 'MADRID', 'SPAIN'),\n",
       "   ('BANGKOK', 'THAILAND', 'MOSCOW', 'RUSSIA'),\n",
       "   ('BANGKOK', 'THAILAND', 'OSLO', 'NORWAY'),\n",
       "   ('BANGKOK', 'THAILAND', 'OTTAWA', 'CANADA'),\n",
       "   ('BANGKOK', 'THAILAND', 'PARIS', 'FRANCE'),\n",
       "   ('BANGKOK', 'THAILAND', 'ROME', 'ITALY'),\n",
       "   ('BANGKOK', 'THAILAND', 'STOCKHOLM', 'SWEDEN'),\n",
       "   ('BANGKOK', 'THAILAND', 'TEHRAN', 'IRAN'),\n",
       "   ('BANGKOK', 'THAILAND', 'TOKYO', 'JAPAN'),\n",
       "   ('BANGKOK', 'THAILAND', 'ATHENS', 'GREECE'),\n",
       "   ('BEIJING', 'CHINA', 'BERLIN', 'GERMANY'),\n",
       "   ('BEIJING', 'CHINA', 'CAIRO', 'EGYPT'),\n",
       "   ('BEIJING', 'CHINA', 'CANBERRA', 'AUSTRALIA'),\n",
       "   ('BEIJING', 'CHINA', 'HAVANA', 'CUBA'),\n",
       "   ('BEIJING', 'CHINA', 'HELSINKI', 'FINLAND'),\n",
       "   ('BEIJING', 'CHINA', 'ISLAMABAD', 'PAKISTAN'),\n",
       "   ('BEIJING', 'CHINA', 'MADRID', 'SPAIN'),\n",
       "   ('BEIJING', 'CHINA', 'MOSCOW', 'RUSSIA'),\n",
       "   ('BEIJING', 'CHINA', 'OSLO', 'NORWAY'),\n",
       "   ('BEIJING', 'CHINA', 'OTTAWA', 'CANADA'),\n",
       "   ('BEIJING', 'CHINA', 'PARIS', 'FRANCE'),\n",
       "   ('BEIJING', 'CHINA', 'ROME', 'ITALY'),\n",
       "   ('BEIJING', 'CHINA', 'STOCKHOLM', 'SWEDEN'),\n",
       "   ('BEIJING', 'CHINA', 'TEHRAN', 'IRAN'),\n",
       "   ('BEIJING', 'CHINA', 'TOKYO', 'JAPAN'),\n",
       "   ('BEIJING', 'CHINA', 'ATHENS', 'GREECE'),\n",
       "   ('BEIJING', 'CHINA', 'BANGKOK', 'THAILAND'),\n",
       "   ('BERLIN', 'GERMANY', 'BERN', 'SWITZERLAND'),\n",
       "   ('BERLIN', 'GERMANY', 'CAIRO', 'EGYPT'),\n",
       "   ('BERLIN', 'GERMANY', 'CANBERRA', 'AUSTRALIA'),\n",
       "   ('BERLIN', 'GERMANY', 'HAVANA', 'CUBA'),\n",
       "   ('BERLIN', 'GERMANY', 'HELSINKI', 'FINLAND'),\n",
       "   ('BERLIN', 'GERMANY', 'ISLAMABAD', 'PAKISTAN'),\n",
       "   ('BERLIN', 'GERMANY', 'MADRID', 'SPAIN'),\n",
       "   ('BERLIN', 'GERMANY', 'MOSCOW', 'RUSSIA')],\n",
       "  'incorrect': [('ATHENS', 'GREECE', 'BAGHDAD', 'IRAQ'),\n",
       "   ('ATHENS', 'GREECE', 'HANOI', 'VIETNAM'),\n",
       "   ('ATHENS', 'GREECE', 'KABUL', 'AFGHANISTAN'),\n",
       "   ('ATHENS', 'GREECE', 'LONDON', 'ENGLAND'),\n",
       "   ('BAGHDAD', 'IRAQ', 'BERN', 'SWITZERLAND'),\n",
       "   ('BAGHDAD', 'IRAQ', 'CANBERRA', 'AUSTRALIA'),\n",
       "   ('BAGHDAD', 'IRAQ', 'LONDON', 'ENGLAND'),\n",
       "   ('BAGHDAD', 'IRAQ', 'OTTAWA', 'CANADA'),\n",
       "   ('BANGKOK', 'THAILAND', 'HANOI', 'VIETNAM'),\n",
       "   ('BANGKOK', 'THAILAND', 'LONDON', 'ENGLAND'),\n",
       "   ('BANGKOK', 'THAILAND', 'BAGHDAD', 'IRAQ'),\n",
       "   ('BEIJING', 'CHINA', 'BERN', 'SWITZERLAND'),\n",
       "   ('BEIJING', 'CHINA', 'HANOI', 'VIETNAM'),\n",
       "   ('BEIJING', 'CHINA', 'KABUL', 'AFGHANISTAN'),\n",
       "   ('BEIJING', 'CHINA', 'LONDON', 'ENGLAND'),\n",
       "   ('BEIJING', 'CHINA', 'BAGHDAD', 'IRAQ'),\n",
       "   ('BERLIN', 'GERMANY', 'HANOI', 'VIETNAM'),\n",
       "   ('BERLIN', 'GERMANY', 'KABUL', 'AFGHANISTAN'),\n",
       "   ('BERLIN', 'GERMANY', 'LONDON', 'ENGLAND')]}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6886371d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to solve the analogies: 2.37 seconds\n"
     ]
    }
   ],
   "source": [
    "# Calculate the time taken\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "print(f\"Time taken to solve the analogies: {time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a23079e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of analogy tasks: 99\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Count analogy tasks\n",
    "num_tasks = 0\n",
    "\n",
    "with open(output_file, \"r\") as file:\n",
    "    for line in file:\n",
    "        if not line.startswith(\":\"):  # Ignore category headers\n",
    "            num_tasks += 1\n",
    "\n",
    "print(f\"Total number of analogy tasks: {num_tasks}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c703c9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly solved analogies: 160\n",
      "Incorrectly solved analogies: 38\n"
     ]
    }
   ],
   "source": [
    "# Count correct and incorrect analogies\n",
    "correct = sum(len(section[\"correct\"]) for section in sections)\n",
    "incorrect = sum(len(section[\"incorrect\"]) for section in sections)\n",
    "\n",
    "print(f\"Correctly solved analogies: {correct}\")\n",
    "print(f\"Incorrectly solved analogies: {incorrect}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8da425e",
   "metadata": {},
   "source": [
    "**2b.** Please answer in writing the following questions:\n",
    "* What is the meaning of the first line of `questions-words-100.txt`?\n",
    "* How many analogies are there in the original `questions-words.txt`?\n",
    "* How much time would it take to solve the original set of analogies?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922f4534",
   "metadata": {},
   "source": [
    "### What is the meaning of the first line of `questions-words-100.txt`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3365bc4c",
   "metadata": {},
   "source": [
    "The first line of the questions-words-100.txt file is typically a category header that indicates the type of analogy questions that follow.  \n",
    "': capital-common-countries' refers to a category of analogy tasks where the analogy is between capital cities of countries.  \n",
    "\n",
    "Example analogy: \"Paris is to France as Berlin is to Germany.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "66241a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of analogy tasks in original file: 19544\n"
     ]
    }
   ],
   "source": [
    "# Please write your answers here.\n",
    "# analogies in the original `questions-words.txt`\n",
    "\n",
    "# Count analogy tasks\n",
    "num_tasks_original = 0\n",
    "\n",
    "with open(input_file, \"r\") as file:\n",
    "    for line in file:\n",
    "        if not line.startswith(\":\"):  # Ignore category headers\n",
    "            num_tasks_original += 1\n",
    "\n",
    "print(f\"Total number of analogy tasks in original file: {num_tasks_original}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f5b93348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analogy score: 0.7401\n"
     ]
    }
   ],
   "source": [
    "# evaluations task on original file\n",
    "# Record start time\n",
    "start_time_og = time.time()\n",
    "\n",
    "analogy_scores_og, sections_og = wv_model.evaluate_word_analogies(input_file)\n",
    "\n",
    "# Record end time\n",
    "end_time_og = time.time()\n",
    "\n",
    "# accuracy returened by evalute_word_analogies\n",
    "print(f'Analogy score: {analogy_scores_og:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "091b8ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to solve the analogies: 6.83 minutes\n"
     ]
    }
   ],
   "source": [
    "# Calculate the time taken\n",
    "time_taken_og = end_time_og - start_time_og\n",
    "\n",
    "print(f\"Time taken to solve the analogies: {time_taken_og/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ea5d4",
   "metadata": {},
   "source": [
    "**2c.** The built-in function from Gensim has several weaknesses, which you will address here.  Please copy the source code of the function `evaluate_word_analogies` from the file `gensim\\models\\keyedvectors.py` and create here a new function which will improve the built-in one as follows.  The function will be called `my_evaluate_word_analogies` and you will also pass it the model as the first argument.  Overall, please proceed gradually and only make minimal modifications, to ensure you don't break the function.  It is important to first understand the structure of the result, `analogies_scores` and `sections`. \n",
    "\n",
    "* Modify the line where `section[incorrect]` is assembled in order to also add to each analogy the *incorrect guess* (i.e. what the model thought was the good answer, but got it wrong).\n",
    "\n",
    "* Modify the code so that when `section[incorrect]` is assembled, you also add the *rank of the correct answer* among the candidates returned by the system (after the incorrect guess).  If the correct answer is not present at all, then code the rank as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e054d8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def my_evaluate_word_analogies(model, analogies, restrict_vocab=300000, case_insensitive=True):\n",
    "def my_evaluate_word_analogies(\n",
    "            wv_model, analogies, restrict_vocab=300000, case_insensitive=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    analogies : str\n",
    "        Path to file, where lines are 4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
    "        See `gensim/test/test_data/questions-words.txt` as example.\n",
    "    restrict_vocab : int, optional\n",
    "        Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
    "        This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
    "        in modern word embedding models).\n",
    "    case_insensitive : bool, optional\n",
    "        If True - convert all words to their uppercase form before evaluating the performance.\n",
    "        Useful to handle case-mismatch between training tokens and words in the test set.\n",
    "        In case of multiple case variants of a single word, the vector for the first occurrence\n",
    "        (also the most frequent if vocabulary is sorted) is taken.\n",
    "    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : float\n",
    "        The overall evaluation score on the entire evaluation set\n",
    "        sections : list of dict of {str : str or list of tuple of (str, str, str, str)}\n",
    "        Results broken down by each section of the evaluation set. Each dict contains the name of the section\n",
    "        under the key 'section', and lists of correctly and incorrectly predicted 4-tuples of words under the\n",
    "        keys 'correct' and 'incorrect'.\n",
    "\n",
    "    \"\"\"\n",
    "    ok_keys = wv_model.index_to_key[:restrict_vocab]\n",
    "\n",
    "    if case_insensitive:\n",
    "        ok_vocab = {k.upper(): wv_model.get_index(k) for k in reversed(ok_keys)}\n",
    "    else:\n",
    "        ok_vocab = {k: wv_model.get_index(k) for k in reversed(ok_keys)}\n",
    "    oov = 0\n",
    "\n",
    "    sections, section = [], None\n",
    "    quadruplets_no = 0\n",
    "\n",
    "    with utils.open(analogies, 'rb') as fin:\n",
    "        for line_no, line in enumerate(fin):\n",
    "            line = utils.to_unicode(line)\n",
    "\n",
    "            if line.startswith(': '):\n",
    "                if section:\n",
    "                    sections.append(section)\n",
    "                section = {'section': line.lstrip(': ').strip(), 'correct': [], 'incorrect': []}\n",
    "            else:\n",
    "                if not section:\n",
    "                    raise ValueError(\"Missing section header before line #%i in %s\" % (line_no, analogies))\n",
    "                \n",
    "                try:\n",
    "                    if case_insensitive:\n",
    "                        a, b, c, expected = [word.upper() for word in line.split()]\n",
    "                    else:\n",
    "                        a, b, c, expected = [word for word in line.split()]\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "                quadruplets_no += 1\n",
    "                if a not in ok_vocab or b not in ok_vocab or c not in ok_vocab or expected not in ok_vocab:\n",
    "                    oov += 1\n",
    "                    section['incorrect'].append((a, b, c, expected))\n",
    "                    continue\n",
    "\n",
    "                original_key_to_index = wv_model.key_to_index\n",
    "                wv_model.key_to_index = ok_vocab\n",
    "\n",
    "                ignore = {a, b, c}  # input words to be ignored\n",
    "                predicted = None\n",
    "                rank = 0\n",
    "\n",
    "                sims = wv_model.most_similar(positive=[b, c], negative=[a], topn=5, restrict_vocab=restrict_vocab)\n",
    "                wv_model.key_to_index = original_key_to_index\n",
    "\n",
    "                for i, element in enumerate(sims, start=1):\n",
    "                    candidate = element[0].upper() if case_insensitive else element[0]\n",
    "                    if candidate in ok_vocab and candidate not in ignore:\n",
    "                        predicted = candidate\n",
    "                        if predicted == expected:\n",
    "                            rank = i\n",
    "                        break\n",
    "\n",
    "                if predicted == expected:\n",
    "                    section['correct'].append((a, b, c, expected))\n",
    "                else:\n",
    "                    correct_rank = next((i for i, elem in enumerate(sims, start=1) if elem[0] == expected), 0)\n",
    "                    section['incorrect'].append((a, b, c, expected, predicted, correct_rank))\n",
    "    if section:\n",
    "        # store the last section, too\n",
    "        sections.append(section)\n",
    "\n",
    "    total = {\n",
    "        'section': 'Total accuracy',\n",
    "        'correct': list(itertools.chain.from_iterable(s['correct'] for s in sections)),\n",
    "        'incorrect': list(itertools.chain.from_iterable(s['incorrect'] for s in sections)),\n",
    "    }\n",
    "\n",
    "    oov_ratio = float(oov) / quadruplets_no * 100 if quadruplets_no > 0 else 0\n",
    "\n",
    "    analogies_score = len(total['correct']) / (len(total['correct']) + len(total['incorrect'])) if (len(total['correct']) + len(total['incorrect'])) > 0 else 0\n",
    "   \n",
    "    sections.append(total)\n",
    "    # Return the overall score and the full lists of correct and incorrect analogies\n",
    "    return analogies_score, sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062fec19",
   "metadata": {},
   "source": [
    "**2d.** Please run the `my_evaluate_word_analogies` function on `questions-words-100.txt` and then write instructions to display, from the results stored in `analogy_scores`:\n",
    "* one incorrectly-solved analogy (selected at random), including also the error made by the model and the rank of the correct answer, thus adding:\n",
    "  - a fifth word, which is the incorrect one found by the model\n",
    "  - a sixth term, which is the integer indicating the rank (or 0)\n",
    "* one correctly-solved analogy selected at random (in principle, four terms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "composite-fundamentals",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analogy Score: 0.8081\n",
      "Total correct: 80\n",
      "Total incorrect: 19\n"
     ]
    }
   ],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "# running my_evaluate_word_analogies function on output text file\n",
    "\n",
    "scores, results = my_evaluate_word_analogies(wv_model, output_file)\n",
    "\n",
    "# Print results\n",
    "print(f\"Analogy Score: {scores:.4f}\")\n",
    "print(f\"Total correct: {len(results[-1]['correct'])}\")\n",
    "print(f\"Total incorrect: {len(results[-1]['incorrect'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3d81a7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Analogy: BAGHDAD, IRAQ, OTTAWA → CANADA (Predicted: PRIME_MINISTER_JEAN_CHRÉTIEN, Rank: 0)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" one incorrectly-solved analogy (selected at random), including also the error made by the model and the rank of the correct answer, thus adding:\n",
    "  - a fifth word, which is the incorrect one found by the model\n",
    "  - a sixth term, which is the integer indicating the rank (or 0) \"\"\"\n",
    "\n",
    "# Extract incorrect analogies\n",
    "incorrect_analogies = results[-1]['incorrect']\n",
    "\n",
    "# Select one randomly (if any exist)\n",
    "if incorrect_analogies:\n",
    "    random_incorrect = random.choice(incorrect_analogies)\n",
    "    print(f\"Incorrect Analogy: {random_incorrect[0]}, {random_incorrect[1]}, {random_incorrect[2]} → {random_incorrect[3]} (Predicted: {random_incorrect[4]}, Rank: {random_incorrect[5]})\")\n",
    "else:\n",
    "    print(\"No incorrect analogies found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a5abeb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Analogy: BANGKOK, THAILAND, PARIS → FRANCE\n"
     ]
    }
   ],
   "source": [
    "# Extract one correctly-solved analogy selected at random (in principle, four terms).\n",
    "correct_analogies = results[-1]['correct']\n",
    "\n",
    "# Select one randomly (if any exist)\n",
    "if correct_analogies:\n",
    "    random_correct = random.choice(correct_analogies)\n",
    "    print(f\"Correct Analogy: {random_correct[0]}, {random_correct[1]}, {random_correct[2]} → {random_correct[3]}\")\n",
    "else:\n",
    "    print(\"No correct analogies found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-contest",
   "metadata": {},
   "source": [
    "**2e.** Please write a function to compute the MRR score given a structure with correctly and incorrectly solved analogies, such as the one that is found in the results from `evaluate_word_analogies`.  The structure is not divided into categories.\n",
    "\n",
    "The Mean Reciprocal Rank (please use the [formula here](https://en.wikipedia.org/wiki/Mean_reciprocal_rank)) gives some credit for incorrectly solved analogies, in inverse proportion to the rank of the correct answer among the candidates.  This rank is 1 for correctly solved analogies (full credit), and 1/k (or 0) for incorrectly solved ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f5dc33e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please define here the function that computes MRR from the information stored in analogy_scores\n",
    "def myMRR(analogies):\n",
    "    \"\"\"\n",
    "    Compute the Mean Reciprocal Rank (MRR) from the analogy evaluation results.\n",
    "\n",
    "    Parameters:\n",
    "    - analogies: A dictionary containing 'correct' and 'incorrect' analogies.\n",
    "\n",
    "    Returns:\n",
    "    - MRR score (float)\n",
    "    \"\"\"\n",
    "    ranks = []\n",
    "\n",
    "    # Process correct analogies (full credit, rank = 1)\n",
    "    for analogy in analogies['correct']:\n",
    "        ranks.append(1.0)\n",
    "\n",
    "    # Process incorrect analogies (credit depends on rank)\n",
    "    for analogy in analogies['incorrect']:\n",
    "        rank = analogy[5]  # The rank of the correct answer\n",
    "        ranks.append(1.0 / rank if rank > 0 else 0.0)\n",
    "\n",
    "    # Compute MRR\n",
    "    return sum(ranks) / len(ranks) if ranks else 0.0  # Avoid division by zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9e928ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy_scores = (scores, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "primary-breach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of analogies: 99\n",
      "Total number of categories: 1\n",
      "Overall accuracy: 0.81 and MRR: 0.81\n"
     ]
    }
   ],
   "source": [
    "# Please test your MRR function by running the following code, which  displays the total number of analogy tasks, \n",
    "# the number of different categories (sections), the accuracy of the results (total number of correctly \n",
    "# solved analogies), and the MRR score of the results:\n",
    "print(\"Total number of analogies:\",  # The last dictionary is the total\n",
    "      len(analogy_scores[1][-1]['correct']) + \n",
    "      len(analogy_scores[1][-1]['incorrect']))\n",
    "print(\"Total number of categories:\", len(analogy_scores[1]) - 1) # the \"total\" is excluded \n",
    "print(f\"Overall accuracy: {analogy_scores[0]:.2f} and MRR: {myMRR(analogy_scores[1][-1]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd4662a",
   "metadata": {},
   "source": [
    "**2f.** Please compute now the accuracy and MRR and the total time for the entire `questions-words.txt` file.  Is the timing compatible with your estimate from (2b)?  What do you think about the difference between accuracy and MRR? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2e0ce720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "\n",
    "# Function to compute Mean Reciprocal Rank (MRR)\n",
    "def compute_mrr(results):\n",
    "    reciprocal_ranks = []\n",
    "    for analogy in results['correct']:\n",
    "        reciprocal_ranks.append(1)  # Correct answers have rank 1\n",
    "        \n",
    "    # Handle incorrectly solved analogies\n",
    "    for analogy in results['incorrect']:\n",
    "        if len(analogy) >= 6:\n",
    "            rank = analogy[5] # Extract the rank of the correct answer\n",
    "            reciprocal_ranks.append(1 / rank if rank > 0 else 0)\n",
    "        else:\n",
    "            reciprocal_ranks.append(0)\n",
    "    return sum(reciprocal_ranks) / len(reciprocal_ranks) if reciprocal_ranks else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "07f2842d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of analogies: 19544\n",
      "Total number of categories: 14\n",
      "Overall accuracy: 0.7320\n",
      "MRR Score: 0.7320\n",
      "Total time taken: 412.27 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Start timing\n",
    "start_time_og_mrr = time.time()\n",
    "\n",
    "# Run evaluation on the full analogy dataset\n",
    "analogy_scores_og_mrr = my_evaluate_word_analogies(wv_model, input_file)\n",
    "\n",
    "# End timing\n",
    "end_time_og_mrr = time.time()\n",
    "total_time_og_mrr = end_time_og_mrr - start_time_og_mrr  # Compute total execution time\n",
    "\n",
    "# Extract accuracy and MRR\n",
    "accuracy_score_og_mrr, sections_og_mrr = analogy_scores_og_mrr\n",
    "total_section_og_mrr = sections_og_mrr[-1]  # The last section contains total results\n",
    "mrr_score = compute_mrr(total_section_og_mrr)\n",
    "\n",
    "# Print results\n",
    "print(\"Total number of analogies:\", len(total_section_og_mrr['correct']) + len(total_section_og_mrr['incorrect']))\n",
    "print(\"Total number of categories:\", len(sections_og_mrr) - 1)  # Excluding the \"Total accuracy\" section\n",
    "print(f\"Overall accuracy: {accuracy_score_og_mrr:.4f}\")\n",
    "print(f\"MRR Score: {mrr_score:.4f}\")\n",
    "print(f\"Total time taken: {total_time_og_mrr:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ee868cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time taken: 6.87 minutes\n"
     ]
    }
   ],
   "source": [
    "print(f'total time taken: {total_time_og_mrr/60:.2f} minutes') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb33ec81",
   "metadata": {},
   "source": [
    "The total time for 'questions-words.txt' when using the function 'evaluate_word_analogies' is 6.83 minutes,  \n",
    "while total time for same original file when using function 'my_evaluate_word_analogies' and using MRR score is 6.87 minutes. Hence there is not much difference.  \n",
    "Overall accuracy: 0.7320 and MRR Score: 0.7320 are also same so no difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-shore",
   "metadata": {},
   "source": [
    "## End of AdvNLP Lab 4\n",
    "Please submit your lab report as a .ipynb file after you have fully run and checked it in Google Colab; then upload it to Moodle.\n",
    "Please submit one notebook per group only and do not forget to put the last names of all team members in the filename."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
